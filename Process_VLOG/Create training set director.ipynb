{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook can process VLOG data to a dataset to be used by DIRECTOR based on a director configuration file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-28T09:34:50.795123Z",
     "start_time": "2020-12-28T09:34:50.556123Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import csv\n",
    "import json\n",
    "import gzip\n",
    "import shutil\n",
    "\n",
    "from vlog_blocks import Tijd_referentie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-28T09:34:50.843115Z",
     "start_time": "2020-12-28T09:34:50.796122Z"
    }
   },
   "outputs": [],
   "source": [
    "#functions used:\n",
    "def load_file(filepath):\n",
    "    with open(filepath, 'r') as fp:\n",
    "        content = fp.read()\n",
    "    return content.split('\\n')\n",
    "\n",
    "def get_delta_time(timeHexString):\n",
    "        \"\"\"\n",
    "        Returns the delta time in deciseconds based on the provided vlog hexadecimal string.\n",
    "\n",
    "        :param str timeHexString: hexadecimal string containing delta time in vlog format\n",
    "        \"\"\"\n",
    "        if (len(timeHexString) != 4):\n",
    "            print(\"Got invalid timeHexString.\")\n",
    "            return None\n",
    "        else:\n",
    "            tenths_of_seconds = int(timeHexString[0:3], 16)\n",
    "\n",
    "            return tenths_of_seconds\n",
    "        \n",
    "def process_detection(message,detector_messages,time_basis):\n",
    "    #process detection\n",
    "    delta_time = get_delta_time(message[2:6])\n",
    "    try:\n",
    "        number_of_detectors = int(message[6:8], 16)\n",
    "        for i in range(number_of_detectors):\n",
    "\n",
    "                state = int(message[8 + i], 16)\n",
    "                time = time_basis+delta_time\n",
    "                if i not in detector_messages.keys():\n",
    "                    detector_messages[i]={\"times\":[],\"states\":[]}\n",
    "                if state<2:\n",
    "                    detector_messages[i]['times'].append(time)\n",
    "                    detector_messages[i]['states'].append(state)\n",
    "    except:\n",
    "        print('detection init error')\n",
    "        pass\n",
    "    return detector_messages\n",
    "\n",
    "def update_detection(message,detector_messages,time_basis):\n",
    "    try:\n",
    "        delta_time=get_delta_time(message[2:5]+\"0\")\n",
    "        number_of_detectors = int((\"0\" + message[5]), 16)\n",
    "\n",
    "        for i in range(number_of_detectors):\n",
    "            detector_index = int(message[6 + (i*2):8 + (i*2)], 16)\n",
    "            state = int(message[8 + (i*2):10 + (i*2)], 16)\n",
    "            if state<2:\n",
    "                time = time_basis+delta_time\n",
    "                detector_messages[detector_index]['times'].append(time)\n",
    "                detector_messages[detector_index]['states'].append(state)\n",
    "    except:\n",
    "        print('detection update error')\n",
    "        pass\n",
    "    return detector_messages\n",
    "#process wus:\n",
    "def process_wus(message,wus_info,time_basis):\n",
    "    #signal states:\n",
    "        signaal_groep_status_type = {\n",
    "        0: 'R',\n",
    "        1: 'G',\n",
    "        2: 'Y',\n",
    "        3: 'WitKnipperen',\n",
    "        4: 'Gedoofd',\n",
    "        5: 'GeelKnipperen'}\n",
    "        # init WUS (signalgroups)\n",
    "        delta_time = get_delta_time(message[2:6])\n",
    "        # Determine number of signal groups\n",
    "        number_of_signal_groups = int(message[6:8], 16)\n",
    "        # Read signal group states\n",
    "        for i in range(number_of_signal_groups):\n",
    "            status_code = int(message[8 + i], 16)\n",
    "            time = time_basis+delta_time\n",
    "            if status_code<3:\n",
    "                status = signaal_groep_status_type[status_code]\n",
    "                if i not in wus_info.keys():\n",
    "                    wus_info[i]={'states':[],\"times\":[]}\n",
    "                wus_info[i]['states'].append(status)\n",
    "                wus_info[i]['times'].append(time)\n",
    "                \n",
    "        return wus_info   \n",
    "def update_wus(message,wus_info,time_basis):\n",
    "    #signal states:\n",
    "        signaal_groep_status_type = {\n",
    "        0: 'R',\n",
    "        1: 'G',\n",
    "        2: 'Y',\n",
    "        3: 'WitKnipperen',\n",
    "        4: 'Gedoofd',\n",
    "        5: 'GeelKnipperen'}\n",
    "        # Determine delta time\n",
    "        delta_time = get_delta_time(message[2:5] + \"0\")\n",
    "        try:\n",
    "            # Determine number of signal groups\n",
    "            number_of_signal_groups = int((\"0\" + message[5]), 16)\n",
    "\n",
    "            # Read signal group states\n",
    "\n",
    "            for i in range(number_of_signal_groups):\n",
    "                    signal_group_index = int(message[6 + (i * 4):8 + (i * 4)], 16)\n",
    "                    status_code = int(message[8 + (i * 4):10 + (i * 4):], 16)\n",
    "                    if status_code<3:\n",
    "                        status = signaal_groep_status_type[status_code]\n",
    "                        time = time_basis+delta_time\n",
    "                        wus_info[signal_group_index]['states'].append(status)\n",
    "                        wus_info[signal_group_index]['times'].append(time)\n",
    "        except:\n",
    "            print('wus update error')\n",
    "            pass\n",
    "        return wus_info\n",
    "def process_messages(vlog_messages):\n",
    "    \"\"\"function to process an array of vlog messages\"\"\"\n",
    "    #set start time to be 14-09-2019\n",
    "    starttijd = \"2018-09-14 00:00:00\"\n",
    "    detector_messages={}\n",
    "    wus_info={}\n",
    "    for message in vlog_messages:\n",
    "        message_id = message[:2]\n",
    "        if message_id ==\"01\":\n",
    "            if message[:4]==\"0120\": #time messages start with ID 01 and start of year prefix 20..(18/19 etc)\n",
    "                time_processor = Tijd_referentie()\n",
    "                time_processor.process_data(message)\n",
    "                time_stamp = time_processor.tijd\n",
    "                if type(time_stamp)!=None:\n",
    "                    delta = np.datetime64(time_stamp) - np.datetime64(starttijd)\n",
    "                    delta = delta / np.timedelta64(1, 's')\n",
    "                #always relate time basis to starting time and most recent time_stamp\n",
    "                    time_basis = int(delta)*10\n",
    "                else:\n",
    "                    print('manual timeupgrade')\n",
    "                    time_basis+=3000 #very wacky way; no way to be sure this is what happened.\n",
    "        elif message_id ==\"05\":\n",
    "            detector_messages = process_detection(message,detector_messages,time_basis)\n",
    "        elif message_id ==\"06\":\n",
    "            detector_messages = update_detection(message,detector_messages,time_basis)\n",
    "\n",
    "        elif (message_id == \"0D\"):\n",
    "            wus_info = process_wus(message,wus_info,time_basis)\n",
    "        elif (message_id == \"0E\"):\n",
    "            wus_info = update_wus(message,wus_info,time_basis)\n",
    "        else:\n",
    "            #unimportant message\n",
    "            pass\n",
    "    return detector_messages, wus_info\n",
    "\n",
    "def write_txt_file(info_dict,intersection,sensor_id, sensor_type):\n",
    "    \"\"\"function to write a text file\"\"\"\n",
    "    if sensor_type=='signal_groups':\n",
    "        name = \"NH_{}.SG{}\".format(intersection, sensor_id)\n",
    "    else:\n",
    "        name = \"NH_{}.{}\".format(intersection, str(sensor_id).rjust(3,'0'))\n",
    "    if not os.path.exists('./output/{}/{}'.format(intersection,sensor_type)):\n",
    "        os.makedirs('./output/{}/{}'.format(intersection,sensor_type))\n",
    "    with open('./output/{}/{}/{}.txt'.format(intersection,sensor_type,name), 'w', newline='') as csvfile:\n",
    "        spamwriter = csv.writer(csvfile, delimiter='\\t')\n",
    "        spamwriter.writerow(['starttijd',name])\n",
    "        for i in range(len(info_dict['times'])):\n",
    "            spamwriter.writerow([info_dict['times'][i],info_dict['states'][i]])\n",
    "        csvfile.close()\n",
    "    #save also as .gz files:\n",
    "    with open('./output/{}/{}/{}.txt'.format(intersection,sensor_type,name), 'rb') as f_in:\n",
    "        with gzip.open(\"./output/{}/{}/{}.txt.gz\".format(intersection,sensor_type,name),'wb') as f_out:\n",
    "                        shutil.copyfileobj(f_in,f_out)\n",
    "            \n",
    "def process_detection_dict(detections,intersection):\n",
    "    \"\"\"process the detection output from process_messages()\"\"\"\n",
    "\n",
    "    sensor_types={\n",
    "        \"arrival_detectors\":{},\n",
    "        \"pedestrians_cyclists\":{},\n",
    "        \"queue_detectors\":{},\n",
    "        \"stopline_detectors\":{}\n",
    "    }\n",
    "    #define approach:\n",
    "    nw=['201231','201225','201212']\n",
    "    n=['205195','205191','205250']\n",
    "    se=['201239','201245','201249','201291','201297','201302','201308','201311']\n",
    "    if intersection in nw:\n",
    "        approach =\"0\"\n",
    "    elif intersection in n:\n",
    "        approach =\"1\"\n",
    "    elif intersection in se:\n",
    "        approach =\"2\"\n",
    "        \n",
    "    #sensor_info_path = r\"X://drive_tom//Msc Datascience//Year 2//Thesis//Siemens//experimenten//NH_201234//PNH_Tom//\"\n",
    "    sensor_info_path = r\"D://uni_drive//Msc Datascience//Year 2//Thesis//Siemens//experimenten//NH_201234//PNH_Tom//\"\n",
    "    sensor_file=open(sensor_info_path+intersection+'.cfg','r').read().split('\\n')\n",
    "    #director_config_path = r\"C:\\Users\\Siemens.MBR-WS-W10\\Downloads\\Director\\Configuration\\201234\\config.json\"\n",
    "    director_config_path= r\"D:\\uni_drive\\Msc Datascience\\Year 2\\Thesis\\Siemens\\experimenten\\NH_201234\\Outlier_Analysis\\Framework\\FPD-LOF\\director_configs.json\"\n",
    "    director_configs =json.load(open(director_config_path, 'rb'))\n",
    "    if 'preceding_intersections' in director_configs['approaches'][approach]:\n",
    "        if intersection in [x['id'] for x in director_configs['approaches'][approach]['preceding_intersections']]:\n",
    "            for itsc in director_configs['approaches'][approach]['preceding_intersections']:\n",
    "                if itsc['id'] == intersection:\n",
    "                    try:\n",
    "                        stopline = itsc['feeding_stopline_detectors']\n",
    "                    except:\n",
    "                        print('problem with stopline detectors {}'.format(intersection))\n",
    "                        print(itsc)\n",
    "                    try:\n",
    "                        queue = itsc['feeding_queue_detectors']\n",
    "                    except:\n",
    "                        print('problem with queue detectors {}'.format(intersection))\n",
    "                        print(itsc)\n",
    "                    try:\n",
    "                        arrivals= itsc['feeding_arrival_detectors']\n",
    "                    except:\n",
    "                        print('problem with arrival detectors {}'.format(intersection))\n",
    "                        print(itsc)\n",
    "    else:\n",
    "        print(\"not in preceding intersections\")\n",
    "    for line in sensor_file:\n",
    "        if line.startswith('DP'):\n",
    "            line =line.split(',')\n",
    "            if line[2].strip('\"\"') in stopline:\n",
    "                sensor_types['stopline_detectors'][line[2].strip('\"\"')]=line[1]\n",
    "            if line[2].strip('\"\"') in queue:\n",
    "                sensor_types['queue_detectors'][line[2].strip('\"\"')]=line[1]\n",
    "            if line[2].strip('\"\"') in arrivals:\n",
    "                sensor_types['arrival_detectors'][line[2].strip('\"\"')]=line[1]\n",
    "    print(\"Sensor type dict:\")\n",
    "    print(sensor_types)\n",
    "    for stopline_detector in sensor_types['stopline_detectors'].keys():\n",
    "        write_txt_file(detections[int(sensor_types['stopline_detectors'][stopline_detector])],intersection, stopline_detector,'stopline_detectors')\n",
    "        \n",
    "    for arrival_detector in sensor_types['arrival_detectors'].keys():\n",
    "        write_txt_file(detections[int(sensor_types['arrival_detectors'][arrival_detector])],intersection, arrival_detector,'arrival_detectors')\n",
    "        \n",
    "    for queue_detector in sensor_types['queue_detectors'].keys():\n",
    "        write_txt_file(detections[int(sensor_types['queue_detectors'][queue_detector])],intersection, queue_detector,'queue_detectors')\n",
    "        \n",
    "    return #nothing for now\n",
    "\n",
    "def process_wus_dict(wus,intersection):\n",
    "\n",
    "    #define approach:\n",
    "    nw=['201231','201225','201212']\n",
    "    n=['205195','205191','205250']\n",
    "    se=['201239','201245','201249','201291','201297','201302','201308','201311']\n",
    "    if intersection in nw:\n",
    "        approach =\"0\"\n",
    "    elif intersection in n:\n",
    "        approach =\"1\"\n",
    "    elif intersection in se:\n",
    "        approach =\"2\"\n",
    "    #LOAD SENSOR DICT AND CONFIGS:\n",
    "    #sensor_info_path = r\"X://drive_tom//Msc Datascience//Year 2//Thesis//Siemens//experimenten//NH_201234//PNH_Tom//\"\n",
    "    sensor_info_path = r\"D://uni_drive//Msc Datascience//Year 2//Thesis//Siemens//experimenten//NH_201234//PNH_Tom//\"\n",
    "    sensor_file=open(sensor_info_path+intersection+'.cfg','r').read().split('\\n')\n",
    "    #director_config_path = r\"C:\\Users\\Siemens.MBR-WS-W10\\Downloads\\Director\\Configuration\\201234\\config.json\"\n",
    "    director_config_path= r\"D:\\uni_drive\\Msc Datascience\\Year 2\\Thesis\\Siemens\\experimenten\\NH_201234\\Outlier_Analysis\\Framework\\FPD-LOF\\director_configs.json\"\n",
    "    director_configs =json.load(open(director_config_path, 'rb'))\n",
    "    \n",
    "    signal_groups=[]\n",
    "    \n",
    "    #SELECT SIGNAL GROUPS:\n",
    "    if 'preceding_intersections' in director_configs['approaches'][approach]:\n",
    "        if intersection in [x['id'] for x in director_configs['approaches'][approach]['preceding_intersections']]:\n",
    "            for itsc in director_configs['approaches'][approach]['preceding_intersections']:\n",
    "                if itsc['id'] == intersection:\n",
    "                    signal_groups += itsc['feeding_signal_groups']\n",
    "    else:\n",
    "        print(\"not in preceding intersections\")\n",
    "    signals={}\n",
    "    for line in sensor_file:\n",
    "        if line.startswith('FC'):\n",
    "            line =line.split(',')\n",
    "            if line[2].strip('\"\"') in signal_groups:\n",
    "                signals[line[2].strip('\"\"')]=line[1]\n",
    "    print(\"Signal_group dict:\")\n",
    "    print(signals)\n",
    "    for signal_group in signals.keys():\n",
    "        write_txt_file(wus[int(signals[signal_group])],intersection, signal_group,'signal_groups')\n",
    "    \n",
    "    return #return nothing for now\n",
    "    \n",
    "def process_201234(detections,wus):\n",
    "    intersection=\"201234\"\n",
    "    #LOAD SENSOR DICT AND CONFIGS:\n",
    "    sensor_info_path = r\"X://drive_tom//Msc Datascience//Year 2//Thesis//Siemens//experimenten//NH_201234//PNH_Tom//\"\n",
    "    #sensor_info_path = r\"D://uni_drive//Msc Datascience//Year 2//Thesis//Siemens//experimenten//NH_201234//PNH_Tom//\"\n",
    "    sensor_file=open(sensor_info_path+intersection+'.cfg','r').read().split('\\n')\n",
    "    #director_config_path = r\"C:\\Users\\Siemens.MBR-WS-W10\\Downloads\\Director\\Configuration\\201234\\config.json\"\n",
    "    director_config_path= r\"X:\\drive_tom\\Msc Datascience\\Year 2\\Thesis\\Siemens\\experimenten\\NH_201234\\Outlier_Analysis\\Framework\\FPD-LOF\\director_configs.json\"\n",
    "    director_configs =json.load(open(director_config_path, 'rb'))\n",
    "    #define sensor types as dict and sensors to add as arrays:\n",
    "    sensor_types={\n",
    "        \"arrival_detectors\":{},\n",
    "        \"pedestrians_cyclists\":{},\n",
    "        \"queue_detectors\":{},\n",
    "        \"stopline_detectors\":{},\n",
    "        \"pedestrians_cyclists\":{}\n",
    "    }\n",
    "    stopline=[]\n",
    "    arrivals=[]\n",
    "    queue=[]\n",
    "    pedestrians=[]\n",
    "    #init signal groups to be added:\n",
    "    signal_groups=[]\n",
    "    signals={}\n",
    "    #grab sensornames from config file\n",
    "    for approach in director_configs['signal_groups'].keys():\n",
    "\n",
    "        a=approach.rjust(2,'0')\n",
    "        signal_groups.append(a)\n",
    "        #if director_configs['signal_groups'][approach]['traffic_type'] =='vehicle':\n",
    "            #stopline+=director_configs['signal_groups'][approach]['stopline_detectors']\n",
    "            #queue+=director_configs['signal_groups'][approach]['queue_detectors']\n",
    "            #arrivals+=director_configs['signal_groups'][approach]['arrival_detectors']\n",
    "        #else: #bikelane or crosswalk\n",
    "            #pedestrians+=director_configs['signal_groups'][approach]['stopline_detectors']\n",
    "            #pedestrians+=director_configs['signal_groups'][approach]['queue_detectors']\n",
    "            #pedestrians+=director_configs['signal_groups'][approach]['arrival_detectors']\n",
    "            #pedestrians+=director_configs['signal_groups'][approach]['push_button']\n",
    "    #run through sensor file to get sensor indices:\n",
    "    #print(signal_groups)\n",
    "    \n",
    "    #edit to add all sensors\n",
    "    for line in sensor_file:\n",
    "        if line.startswith('DP'):\n",
    "            line =line.split(',')\n",
    "            if len(line[2].strip('\"\"'))==3:\n",
    "                if (int(line[3])==257) & (int(line[2].strip('\"\"'))<200):\n",
    "                    sensor_types['stopline_detectors'][(line[2]).strip('\"\"')]=line[1]\n",
    "                if (int(line[3])==513) & (int(line[2].strip('\"\"'))<200):\n",
    "                    sensor_types['queue_detectors'][(line[2]).strip('\"\"')]=line[1]\n",
    "                if (int(line[3])==1025) & (int(line[2].strip('\"\"'))<200):\n",
    "                    sensor_types['arrival_detectors'][(line[2]).strip('\"\"')]=line[1]\n",
    "                if line[2].strip('\"\"') in ['233','234','243','244','331','341']:\n",
    "                    sensor_types['pedestrians_cyclists'][(line[2]).strip('\"\"')]=line[1]\n",
    "        elif line.startswith('FC'):\n",
    "            line =line.split(',')\n",
    "            if line[2].strip('\"\"') in signal_groups:\n",
    "                signals[(line[2]).strip('\"\"')]=line[1]\n",
    "    print(signals)    \n",
    "    for signal_group in signals.keys():\n",
    "        write_txt_file(wus[int(signals[signal_group])],intersection, signal_group,'signal_groups')\n",
    "            \n",
    "    for stopline_detector in sensor_types['stopline_detectors'].keys():\n",
    "        write_txt_file(detections[int(sensor_types['stopline_detectors'][stopline_detector])],intersection, stopline_detector,'stopline_detectors')\n",
    "        \n",
    "    for arrival_detector in sensor_types['arrival_detectors'].keys():\n",
    "        write_txt_file(detections[int(sensor_types['arrival_detectors'][arrival_detector])],intersection, arrival_detector,'arrival_detectors')\n",
    "        \n",
    "    for queue_detector in sensor_types['queue_detectors'].keys():\n",
    "        write_txt_file(detections[int(sensor_types['queue_detectors'][queue_detector])],intersection, queue_detector,'queue_detectors')\n",
    "    \n",
    "    for pedestrians in sensor_types['pedestrians_cyclists'].keys():\n",
    "        write_txt_file(detections[int(sensor_types['pedestrians_cyclists'][pedestrians])],intersection, pedestrians,'pedestrians_cyclists')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-28T09:34:50.918678Z",
     "start_time": "2020-12-28T09:34:50.915677Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_intersection(intersection):\n",
    "    vlog_messages=[]\n",
    "    for file in os.listdir(r\"./input/Data Boris 16-09-2020 deel 1/\"+intersection):\n",
    "        if file.endswith(\".vlg\"):\n",
    "            vlog_messages += load_file(r\"./input/Data Boris 16-09-2020 deel 1/{}/{}\".format(intersection,file))\n",
    "    detectors, wus = process_messages(vlog_messages)\n",
    "    del vlog_messages\n",
    "    if intersection =='201234':\n",
    "        process_201234(detectors,wus)\n",
    "    else:\n",
    "        #try:\n",
    "            process_detection_dict(detectors,intersection)\n",
    "            process_wus_dict(wus,intersection)\n",
    "        #except:\n",
    "        #    return detectors,wus\n",
    "            del wus\n",
    "            del detectors\n",
    "    return 0,0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-28T09:34:51.137844Z",
     "start_time": "2020-12-28T09:34:51.132844Z"
    }
   },
   "outputs": [],
   "source": [
    "#intersections= ['201234','201225','205191','201239','201249','201291','201308','201225','201231']\n",
    "#intersections=['201234','201225','205191','201239',\n",
    "intersections=['201234']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-28T09:37:35.422672Z",
     "start_time": "2020-12-28T09:34:51.515375Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201234\n",
      "Got invalid timeHexString.\n",
      "detection update error\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "for intersection in intersections:\n",
    "    print(intersection)\n",
    "    process_intersection(intersection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
